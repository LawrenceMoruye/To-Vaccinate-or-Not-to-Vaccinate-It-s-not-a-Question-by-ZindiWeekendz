{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path \n\nimport os\n\nimport torch\nimport torch.optim as optim\n\nimport random \n\n# fastai\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\n\n# transformers\nfrom transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n\nfrom transformers import BertForSequenceClassification, BertTokenizer, BertConfig\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\nfrom transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\nfrom transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig","execution_count":233,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import fastai\nimport transformers\nprint('fastai version :', fastai.__version__)\nprint('transformers version :', transformers.__version__)","execution_count":234,"outputs":[{"output_type":"stream","text":"fastai version : 1.0.60\ntransformers version : 2.7.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":235,"outputs":[{"output_type":"stream","text":"/kaggle/input/zindi-weekend-vaccine/Train.csv\n/kaggle/input/zindi-weekend-vaccine/SampleSubmission.csv\n/kaggle/input/zindi-weekend-vaccine/Test.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_ROOT = Path(\"..\") / \"/kaggle/input/zindi-weekend-vaccine\"\ntrain = pd.read_csv(DATA_ROOT / 'Train.csv')\ntest = pd.read_csv(DATA_ROOT / 'Test.csv')\nprint(train.shape,test.shape)\ntrain.head()","execution_count":236,"outputs":[{"output_type":"stream","text":"(10001, 4) (5177, 2)\n","name":"stdout"},{"output_type":"execute_result","execution_count":236,"data":{"text/plain":"   tweet_id                                          safe_text  label  \\\n0  CL1KWCMY  Me &amp; The Big Homie meanboy3000 #MEANBOY #M...    0.0   \n1  E3303EME  I'm 100% thinking of devoting my career to pro...    1.0   \n2  M4IVFSMS  #whatcausesautism VACCINES, DO NOT VACCINATE Y...   -1.0   \n3  1DR6ROZ4  I mean if they immunize my kid with something ...   -1.0   \n4  J77ENIIE  Thanks to <user> Catch me performing at La Nui...    0.0   \n\n   agreement  \n0        1.0  \n1        1.0  \n2        1.0  \n3        1.0  \n4        1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>safe_text</th>\n      <th>label</th>\n      <th>agreement</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CL1KWCMY</td>\n      <td>Me &amp;amp; The Big Homie meanboy3000 #MEANBOY #M...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>E3303EME</td>\n      <td>I'm 100% thinking of devoting my career to pro...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>M4IVFSMS</td>\n      <td>#whatcausesautism VACCINES, DO NOT VACCINATE Y...</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1DR6ROZ4</td>\n      <td>I mean if they immunize my kid with something ...</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>J77ENIIE</td>\n      <td>Thanks to &lt;user&gt; Catch me performing at La Nui...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train[train['label'].notnull()]","execution_count":237,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":238,"outputs":[{"output_type":"execute_result","execution_count":238,"data":{"text/plain":"(10000, 4)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['label']=train['label'].apply(lambda x:int(x))","execution_count":239,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['label'] = train['label'].map({-1: 0, 0: 1,1:2})","execution_count":240,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain.safe_text = train.safe_text.str.replace(r'http(\\S)+', r'')\ntrain.safe_text = train.safe_text.str.replace(r'http ...', r'')\ntrain.safe_text = train.safe_text.str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\ntrain.safe_text = train.safe_text.str.replace(r'@[\\S]+',r'')\n\n# Remove non-ascii words or characters\n#train.safe_text = [''.join([i if ord(i) < 128 else '' for i in safe_text]) for safe_text in train.safe_text]\ntrain.safe_text = train.safe_text.str.replace(r'_[\\S]?',r'')\n\n# Remove extra space\ntrain.safe_text = train.safe_text.str.replace(r'[ ]{2, }',r' ')\n\n# Remove &, < and >\ntrain.safe_text = train.safe_text.str.replace(r'&amp;?',r'and')\ntrain.safe_text = train.safe_text.str.replace(r'&lt;',r'<')\ntrain.safe_text = train.safe_text.str.replace(r'&gt;',r'>')\n\n# Insert space between words and punctuation marks\ntrain.safe_text = train.safe_text.str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\ntrain.safe_text = train.safe_text.str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n\n# Lowercased and strip\n#train.safe_text = train.safe_text.str.lower()\ntrain.safe_text = train.safe_text.str.strip()\n","execution_count":241,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest.safe_text = test.safe_text.str.replace(r'http(\\S)+', r'')\ntest.safe_text = test.safe_text.str.replace(r'http ...', r'')\ntest.safe_text = test.safe_text.str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\ntest.safe_text = test.safe_text.str.replace(r'@[\\S]+',r'')\n\n# Remove non-ascii words or characters\n#test.safe_text = [''.join([i if ord(i) < 128 else '' for i in safe_text]) for safe_text in test.safe_text]\ntest.safe_text = test.safe_text.str.replace(r'_[\\S]?',r'')\n\n# Remove extra space\ntest.safe_text = test.safe_text.str.replace(r'[ ]{2, }',r' ')\n\n# Remove &, < and >\ntest.safe_text = test.safe_text.str.replace(r'&amp;?',r'and')\ntest.safe_text = test.safe_text.str.replace(r'&lt;',r'<')\ntest.safe_text = test.safe_text.str.replace(r'&gt;',r'>')\n\n# Insert space between words and punctuation marks\ntest.safe_text = test.safe_text.str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\ntest.safe_text = test.safe_text.str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n\n# Lowercased and strip\n#test.safe_text = test.safe_text.str.lower()\ntest.safe_text = test.safe_text.str.strip()\n","execution_count":242,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['safe_text'][0]","execution_count":243,"outputs":[{"output_type":"execute_result","execution_count":243,"data":{"text/plain":"'Me and The Big Homie meanboy3000 # MEANBOY # MB # MBS # MMR # STEGMANLIFE @ Stegman St . < url >'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['safe_text'][1]","execution_count":244,"outputs":[{"output_type":"execute_result","execution_count":244,"data":{"text/plain":"\"I ' m 100 % thinking of devoting my career to proving autism isn ' t caused by vaccines due to the IDIOTIC posts I ' ve seen about World Autism Day\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":245,"outputs":[{"output_type":"execute_result","execution_count":245,"data":{"text/plain":"   tweet_id                                          safe_text  label  \\\n0  CL1KWCMY  Me and The Big Homie meanboy3000 # MEANBOY # M...    0.0   \n1  E3303EME  I ' m 100 % thinking of devoting my career to ...    1.0   \n\n   agreement  \n0        1.0  \n1        1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>safe_text</th>\n      <th>label</th>\n      <th>agreement</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CL1KWCMY</td>\n      <td>Me and The Big Homie meanboy3000 # MEANBOY # M...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>E3303EME</td>\n      <td>I ' m 100 % thinking of devoting my career to ...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text_length'] = [len(safe_text.split(' ')) for safe_text in train.safe_text]","execution_count":246,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train['text_length']>0]\ntrain = train.drop_duplicates(subset=['safe_text'])","execution_count":247,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_CLASSES = {\n    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n}","execution_count":248,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\nseed = 15\nuse_fp16 = False\nbs = 32\n\nmodel_type = 'roberta'\npretrained_model_name = 'roberta-base'\n","execution_count":249,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]","execution_count":250,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_class.pretrained_model_archive_map.keys()","execution_count":251,"outputs":[{"output_type":"execute_result","execution_count":251,"data":{"text/plain":"dict_keys(['roberta-base', 'roberta-large', 'roberta-large-mnli', 'distilroberta-base', 'roberta-base-openai-detector', 'roberta-large-openai-detector'])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_all(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) \n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \n        ","execution_count":252,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_all(seed)","execution_count":253,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransformersBaseTokenizer(BaseTokenizer):\n    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n        self._pretrained_tokenizer = pretrained_tokenizer\n        self.max_seq_len = pretrained_tokenizer.max_len\n        self.model_type = model_type\n\n    def __call__(self, *args, **kwargs): \n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        CLS = self._pretrained_tokenizer.cls_token\n        SEP = self._pretrained_tokenizer.sep_token\n        if self.model_type in ['roberta']:\n            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n            tokens = [CLS] + tokens + [SEP]\n        else:\n            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n            if self.model_type in ['xlnet']:\n                tokens = tokens + [SEP] +  [CLS]\n            else:\n                tokens = [CLS] + tokens + [SEP]\n        return tokens","execution_count":254,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\ntransformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\nfastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])","execution_count":255,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransformersVocab(Vocab):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        super(TransformersVocab, self).__init__(itos = [])\n        self.tokenizer = tokenizer\n    \n    def numericalize(self, t:Collection[str]) -> List[int]:\n        return self.tokenizer.convert_tokens_to_ids(t)\n        #return self.tokenizer.encode(t)\n\n    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n        nums = np.array(nums).tolist()\n        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n    \n    def __getstate__(self):\n        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n\n    def __setstate__(self, state:dict):\n        self.itos = state['itos']\n        self.tokenizer = state['tokenizer']\n        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})","execution_count":256,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\nnumericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n\ntokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n\ntransformer_processor = [tokenize_processor, numericalize_processor]","execution_count":257,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pad_first = bool(model_type in ['xlnet'])\npad_idx = transformer_tokenizer.pad_token_id","execution_count":258,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = transformer_tokenizer.tokenize('This is Lawrence trial')\nprint(tokens)\nids = transformer_tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)\ntransformer_tokenizer.convert_ids_to_tokens(ids)\n","execution_count":259,"outputs":[{"output_type":"stream","text":"['This', 'Ġis', 'ĠLawrence']\n[713, 16, 6226]\n","name":"stdout"},{"output_type":"execute_result","execution_count":259,"data":{"text/plain":"['This', 'Ġis', 'ĠLawrence']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":260,"outputs":[{"output_type":"execute_result","execution_count":260,"data":{"text/plain":"Index(['tweet_id', 'safe_text', 'label', 'agreement', 'text_length'], dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"databunch = (TextList.from_df(train, cols='safe_text', processor=transformer_processor)\n             .split_by_rand_pct(0.1,seed=seed)\n             .label_from_df(cols= 'label')\n             .add_test(test)\n             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))","execution_count":261,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('[CLS] token :', transformer_tokenizer.cls_token)\nprint('[SEP] token :', transformer_tokenizer.sep_token)\nprint('[PAD] token :', transformer_tokenizer.pad_token)\ndatabunch.show_batch()","execution_count":262,"outputs":[{"output_type":"stream","text":"[CLS] token : <s>\n[SEP] token : </s>\n[PAD] token : <pad>\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>&lt;s&gt; Ġç Ĺ ħ é Ļ ¢ å® Ł ç ¿ Ĵ è¡ Į ãģı ãģ® ãģ« MM R ãģ¨ æ° ´ ç ĸ ± ç ĺ ¡ ãģ®æ Ĭ Ĺ ä½ ĵ ãĤĴ èª ¿ ãģ ¹ ãģŁ ãĤī Ġ$ Ġ550 ãģ® è « ĭ æ ± Ĥ ãģĮ æĿ ¥ ãģŁ Ġ ãĢĤ Ċ (( (( ï ¼ Ľ Ġï ¾ Ł Ð Ķ ï ¾ Ł Ġ)</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>&lt;s&gt; ĠFri Ġ12 Ġnoon Ġmeet ĠðŁ İ ī ðŁ Ĵ ¥ ĠGolden ĠGate ĠBridge ĠðŁ İ Ī Ġto Ġsave Ġbabies ĠðŁĳ ¶ ðŁ Ĵ ĵ Ġand Ġstop ĠðŁ Ĵ Ģ ðŁ Ļ Ģ ðŁ Ĵ ī ĠVacc inations ĠðŁ Ĵ ī ðŁ Ķ ª ðŁ Ķ « ĠSB Ġ277 Ġwear Ġyellow Ġ ĠðŁĺ Ĭ ðŁ İ Ī â ļ ¡ ï¸ı ðŁĺ İ ðŁĺ ĺ ðŁĳ ¨ âĢ į ðŁĳ</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>&lt;s&gt; ĠðŁ Ĵ ¥ ðŁ İ § ðŁ İ ¼ ðŁ İ µ ðŁ İ ¶ ðŁ ı ¢ ðŁ Ĵ Ģ ðŁĳ Ĥ ðŁ Ĵ ¥ &lt; Ġuser Ġ&gt; Ġ# Ġmm r Ġ# Ġmix master rod Ġ# Ġd cd j Ġ# Ġscratching Al ittle bit Ġ# Ġmad h Ġ Ġ@ ĠMix Master Rod Ġ' Ġs ĠUp stairs ĠLounge Ġ&lt; Ġurl Ġ&gt; &lt;/s&gt;</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>&lt;s&gt; Ġ&lt; Ġuser Ġ&gt; ĠEst am os Ġvac un ados Ġcontra Ġmuch as Ġcos as Ġper o Ġa Ãº n Ġas ÃŃ Ġhay Ġque Ġp oner se Ġref uer zos Ġde Ġc i ert as Ġvac un as Ġ. ĠRec ient ement e Ġme Ġp use Ġla ĠMMR Ġ. &lt;/s&gt;</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>&lt;s&gt; Ġ&lt; Ġuser Ġ&gt; Ġha br Ã¡ Ġun Ġsoft Ġreset Ġy Ġtend r Ã¡s Ġque Ġj ugar Ġpart idas Ġde Ġpos icion am ient o Ġde Ġac uer do Ġa Ġtu Ġmm r Ġ. ĠNo Ġsab r ÃŃa Ġdec ir te Ġqu Ã© Ġte Ġsu ced er Ã¡ Ġ. &lt;/s&gt;</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('[CLS] id :', transformer_tokenizer.cls_token_id)\nprint('[SEP] id :', transformer_tokenizer.sep_token_id)\nprint('[PAD] id :', pad_idx)\ntest_one_batch = databunch.one_batch()[0]\nprint('Batch shape : ',test_one_batch.shape)\nprint(test_one_batch)","execution_count":263,"outputs":[{"output_type":"stream","text":"[CLS] id : 0\n[SEP] id : 2\n[PAD] id : 1\nBatch shape :  torch.Size([32, 135])\ntensor([[    0, 48283,  6800,  ...,  1437, 45682,     2],\n        [    0,   132,   282,  ...,     1,     1,     1],\n        [    0,    44,    48,  ...,     1,     1,     1],\n        ...,\n        [    0, 28696,  3018,  ...,     1,     1,     1],\n        [    0, 28696,  3018,  ...,     1,     1,     1],\n        [    0,  1491,     7,  ...,     1,     1,     1]])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomTransformerModel(nn.Module):\n    def __init__(self, transformer_model: PreTrainedModel):\n        super(CustomTransformerModel,self).__init__()\n        self.transformer = transformer_model\n        \n    def forward(self, input_ids, attention_mask=None):\n        \n        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) \n        \n        logits = self.transformer(input_ids,\n                                  attention_mask = attention_mask)[0]   \n        return logits","execution_count":264,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = config_class.from_pretrained(pretrained_model_name)\nconfig.num_labels= 3\nconfig.use_bfloat16 = use_fp16\nprint(config)","execution_count":265,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'num_labels' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-265-05468c3adf04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bfloat16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_fp16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'num_labels' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\ncustom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.callbacks import *\nfrom transformers import AdamW\nfrom functools import partial\n\nCustomAdamW = partial(AdamW, correct_bias=False)\n\nlearner = Learner(databunch, \n                  custom_transformer_model, \n                  opt_func = CustomAdamW, \n                  metrics=[accuracy, error_rate])\n\nlearner.callbacks.append(ShowGraph(learner))\n\nif use_fp16: learner = learner.to_fp16()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(learner.model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlist_layers = [learner.model.transformer.roberta.embeddings,\n              learner.model.transformer.roberta.encoder.layer[0],\n              learner.model.transformer.roberta.encoder.layer[1],\n              learner.model.transformer.roberta.encoder.layer[2],\n              learner.model.transformer.roberta.encoder.layer[3],\n              learner.model.transformer.roberta.encoder.layer[4],\n              learner.model.transformer.roberta.encoder.layer[5],\n              learner.model.transformer.roberta.encoder.layer[6],\n              learner.model.transformer.roberta.encoder.layer[7],\n              learner.model.transformer.roberta.encoder.layer[8],\n              learner.model.transformer.roberta.encoder.layer[9],\n              learner.model.transformer.roberta.encoder.layer[10],\n              learner.model.transformer.roberta.encoder.layer[11],\n               \n              learner.model.transformer.roberta.pooler]\n              \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.split(list_layers)\nnum_groups = len(learner.layer_groups)\nprint('Learner split in',num_groups,'groups')\nprint(learner.layer_groups)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.save('untrain')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_all(seed)\nlearner.load('untrain');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.freeze_to(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.recorder.plot(skip_end=10,suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit_one_cycle(1,max_lr=2e-54,moms=(0.8,0.9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.save('first_cycle')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_all(seed)\nlearner.load('first_cycle');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.freeze_to(-2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 1e-5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit_one_cycle(3, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.save('second_cycle')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_all(seed)\nlearner.load('second_cycle');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.freeze_to(-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit_one_cycle(3, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.save('third_cycle')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_all(seed)\nlearner.load('third_cycle');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.unfreeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit_one_cycle(3, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.predict('me and the big homie meanboy3000 # meanboy # mb # mbs # mmr # stegmanlife @ stegman st . < url >')#1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.predict(\"i ' m 100 % thinking of devoting my career to proving autism isn ' t caused by vaccines due to the idiotic posts i ' ve seen about world autism day\")#2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.export(file = 'transformer.pkl');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/working'\nexport_learner = load_learner(path, file = 'transformer.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n  \n    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in databunch.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    return preds[reverse_sampler, :]\n\ntest_preds = get_preds_as_nparray(DatasetType.Test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df=test[['tweet_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df['label']=np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = sub_df\nsample_submission['label'] = np.argmax(test_preds,axis=1)\nsample_submission['label'] = sample_submission['label'].map({0:-1, 1: 0,2:1})\nsample_submission.to_csv(\"allsub7.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}